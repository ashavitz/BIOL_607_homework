---
title: "Midterm Exam"
author: "Aaron Shavitz"
format: html
---
github: https://github.com/ashavitz/BIOL_607_homework/tree/main  

# Midterm Exam!  

### Load Libraries  
All libraries used in the Midterm are loaded here:
```{r, message=FALSE, warning=FALSE}
# Data management
library(dplyr)
library(readr)
library(tidyr)

# Visualization
library(ggplot2)
library(ggthemes)

# Modeling and related
library(broom)
library(broom.mixed)
library(DHARMa)
library(emmeans)
library(glmmTMB)
library(lme4)
library(performance)
library(modelr)
library(modelbased)
```

### Set ggplot theme
```{r}
# Setting a standard ggplot theme for plotting
theme_set(theme_clean())

```

## 1. Data Reshaping and Visuzliation  
**Johns Hopkins has been maintaining one of the best Covid-19 timseries data sets out there. The data on the US can be found here with information about what is in the data at https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data**  


### 1a. Access  
**Download and read in the data. Can you do this without downloading, but read directly from the archive?**  
```{r}
covid_us <- read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/refs/heads/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
```


### 1b. It’s big and wide!  
**The data is, well, huge. It’s also wide, with dates as columns. Write a workflow that will filter to a state (your choice which one!), and then return a time series (long data where every row is a day) with columns for date, new daily cases and for cumulative cases in that state.**  
```{r}
# Filter to include only Vermont
covid_vt <- filter(covid_us, Province_State == "Vermont") |>
  
  # Move state name to first column
  relocate(Province_State, Admin2, .before = UID) |>
  
  # Remove excess variables
  select(-c(UID:Combined_Key)) |>
  
  # Pivot longer to make each row a unique day. Columns for date and cumulative cases
  pivot_longer(cols = -c(Province_State, Admin2),
               names_to = "date_col",
               values_to = "cumulative_cases") |> 
  
  # Convert date_col to Date format
  mutate(date_col = lubridate::mdy(date_col)) |> 
  
  # Group by date to sum values across Admin2 (get whole state values per day)
  group_by(date_col) |>
  summarize(cumulative_cases = sum(cumulative_cases)) |> 
  
  # Calculate new_daily_cases based on cumulative data
  mutate(new_daily_cases = c(cumulative_cases[1], diff(cumulative_cases)))

# Note - I read about diff() here:
# https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/diff 
# and used cumulative_cases[1] as the first value, as the first new value would be the same
# as the first cumulative value
```

**Note, let’s make the date column that emerges a true date object. Let’s say you’ve called it date_col. If you mutate it, `mutate(date_col = lubridate::mdy(date_col))`, it will be turned into a date object that will have a recognized order. {lubridate} is da bomb, and I’m hoping we have some time to cover it in the future.**  

(included in code chunk above)


**Even better - impress yourself (if you want - not required!) and merge it with some other data source to also return cases per 100,000 people.**  
```{r}
# Found US Census Data for 2020 - 2024 here: 
# https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

# Read in data
# census_us <- read_csv("https://www2.census.gov/programs-surveys/popest/datasets/2020-2024/state/totals/NST-EST2024-ALLDATA.csv")
# 
# # Select only relevant data and Filter data to Vermont:
# census_vt <- census_us |> 
#   filter(NAME == "Vermont") |> 
#   select(c("POPESTIMATE2020", "POPESTIMATE2021", "POPESTIMATE2022", "POPESTIMATE2023"))

# Pivot census data long by year

# Create corresponding year column in covid_vt df

# Merge dfs and calculate new 
```



### 1c. Let’s get visual!  
**Great! Make a compelling plot of the timeseries for Massachusetts! Points for style, class, ease of understanding major trends, etc. Note, 10/10 for yourself only for the most killer figures. Don’t phone it in! Also, note what the data from JHU is. Do you want the cumulative, or daily, or what? Want to highlight features? Events? Go wild!**  


```{r}
# Repeat data organization from above, but for MA
covid_ma <- filter(covid_us, Province_State == "Massachusetts") |>
  relocate(Province_State, Admin2, .before = UID) |>
  select(-c(UID:Combined_Key)) |>
  pivot_longer(cols = -c(Province_State, Admin2),
               names_to = "date_col",
               values_to = "cumulative_cases") |> 
  mutate(date_col = lubridate::mdy(date_col)) |>
  group_by(date_col) |>
  summarize(cumulative_cases = sum(cumulative_cases)) |> 
  mutate(new_daily_cases = c(cumulative_cases[1], diff(cumulative_cases)))


# Plot cumulative cases and new cases on same plot. Left y axis is new cases, right is cumulative.
ggplot(data = covid_ma, 
       mapping = aes(x = date_col,
                     y = cumulative_cases)) +
  geom_line(size = 1.5) +
  theme_stata() +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Massachusetts - Cumulative Covid Cases",
       subtitle = "January 2020 - March 2023",
       x = "Date",
       y = "Cumulative Count of Covid Cases") +
  theme(
    axis.text.y = element_text(angle = 0, size = 7)
  ) +
  annotate(
    "text", 
    x = as.Date("2022-02-01"),
    y = 1250000,
    label = "Initial Omicron Wave", 
    vjust = -1, 
    hjust = 1.5,
    size = 3
  ) +
  geom_segment(
    aes(x = as.Date("2021-02-01"),
        xend = as.Date("2022-01-01"),
        y = 1250000,
        yend = 1250000),
    arrow = arrow(type = "closed", length = unit(0.2, "inches")),
    size = 1
  )
  
# For labeling, got help from: https://r-graph-gallery.com/233-add-annotations-on-ggplot2-chart.html
# Identified Omicrom wave from: https://publichealth.jhu.edu/2022/covid-year-in-review

```


## 2. Fit and Evaluate a Linear model  
**Let’s fit and evaluate a linear model! To motivate this, we’ll look at Burness et al.’s 2012 study “Post-hatch heat warms adult beaks: irreversible physiological plasticity in Japanese quail http://rspb.royalsocietypublishing.org/content/280/1767/20131436.short the data for which they have made available at Data Dryad at http://datadryad.org/resource/doi:10.5061/dryad.gs661. We’ll be doing a slightly modified version of their analysis.**  


## 2a. What should I do?  
**Starting with loading this (new to you) data set for the very first time, what are the steps that you would take to analyze this data? Write them out! Maybe even in comments!**  

I plan to:  

1. Load in the data 
2. Examine the data by looking at the structure and View()ing the data (I generally do these in my R Environment rather than by typing out str() or View()). I may also look at summary information 
3. Identify variables of interest and create initial simple visualizations (plots) 
4. Since it says we're fitting a linear model, I would then make this model with my variables of interest and **check its assumptions** 
5. If assumptions look good, I will start to examine what the model is telling us, and if there seem to be takeaways from it. If the model fails assumptions, I would reconsider how I might need to modify the model. 
6. Once I have evaluated my model, I will visualize it with plots to convery useful information from the model.   


## 2b. Let’s get started  
**Load the data. Look at it. Anything interesting? Anything you’d want to watch out for? Or is it OK? Anything you’d change to make working with the data easier? Also, manipulate it a bit - for our analysis, we’re going to want Bird # and the Age of the birds to be categorical variables. Make sure we have variables that will work!**  
```{r}
morph_data <- read_csv("data/Morphology+data.csv")
# morph_data <- read_csv("https://datadryad.org/downloads/file_stream/66259")

# str(morph_data)
# View(morph_data)
# unique(morph_data$`Bird #`)

# convert Bird #, Age, Temp treatment, and Sex to categorical variables (factors)
morph_data <- morph_data |> 
  mutate(
    `Bird #` = as.factor(`Bird #`),
    `Age (days)` = as.factor(`Age (days)`),
    `Exp. Temp. (degree C)` = as.factor(`Exp. Temp. (degree C)`),
    Sex = as.factor(Sex)
  )
```
  
  
To watch out for:  

- We can see there are NA observations for some variables
- Also, I am seeing that there is data for weight and various size measurements. It is logical that these may be correlated, so we should watch out for that when making models.
- The data is long. There are only 40 unique birds, and the measurements appear to be taken at different ages, with 20 birds in one temp treatment and 20 in another.
- As stated, Bird # is initually numeric, but should be categorical.



## 2c. Viz before fit  
**The model we will fit is one where we want to look at how temperature treatment affects the development of tarsus length over time (age). Visualize this. Make it look good! Is there anything here that would shape how you fit a model? Do you see why we are treating age as categorical?**  
```{r}
# Plot tarsus length by age, colored by temp treatment
ggplot(data = morph_data,
       mapping = aes(x = `Age (days)`,
                     y = `Tarsus (mm)`,
                     color = `Exp. Temp. (degree C)`)) +
  geom_jitter(alpha = 0.5) +
  scale_colour_manual(values = c("blue", "red")) + 
  guides(color = guide_legend(reverse = TRUE))

# Plot tarsus length by temperature, colored by age
ggplot(data = morph_data,
       mapping = aes(x = `Exp. Temp. (degree C)`,
                     y = `Tarsus (mm)`,
                     color = `Age (days)`)) +
  geom_jitter(alpha = 0.2, height = 0) +
  geom_boxplot()
```


**Is there anything here that would shape how you fit a model?**  

Around day 34, the nature of the relationship between Tarsus length and age appears to change, leveling out and possibly increasing in spread.  


**Do you see why we are treating age as categorical?**  

I believe we are treating age as categorical because we already assume/know that age will impact tarsus length, but we want to see if the temperature treatment has an impact on tarsus length at different static ages. Thus, we are not treating age as continuous and calculating how change in age impacts tarsus length, rather we are treating age as a treatment category, to see if any age-temperature combinations have an effect on tarsus length.  



## 2d. Fit and evaluate  
**Fit a model where tarsus length is predicted by age, treatment, their interaction, and a ‘block’ effect, as it were, of bird #. Evaluate the fit. Make any modifications as you see necessary due to your model checks. Note, it’s not going to be perfect (I checked the original - hey, you can too - and they’re on the edge) - but our models are robust, so we’re OK.**  

```{r}
tarsus_lm <- lm(`Tarsus (mm)` ~ `Age (days)`*`Exp. Temp. (degree C)` + `Bird #`,
                data = morph_data)

# TODO - PICK UP HERE
check_model(tarsus_lm)
check_collinearity(lm(`Tarsus (mm)` ~ `Age (days)` + `Exp. Temp. (degree C)` + `Bird #`,
                data = morph_data)) |> plot()
```
The primary potential issue in the assumptions check is that there are high reported VIFs, suggesting possible collinearity. However, this will happen when you include interaction in your model, so I tested just collinearity for the model without interaction. We still see high VIFs for temperature and bird #, but this is caveated with "Model matrix is rank deficient. VIFs may not be sensible." I believe this comes from the fact that there are only 2 temperature levels and bird number is repeated labelling, so these are essentially structured combinations leading to misevaluation of collinearity.  
Overall, I'd say the assumptions look good and no model adjustments are needed.  


## 2e. Answer the Question  
**A central question of this paper is - does temperature affect the development of tarsus length. With your fit model in hand, answer the question, however you deem fit. Make sure that, alongside any numerical answers you produce, there is at least one stunning figure for a publishable paper!**  
```{r}
# Evaluate the model
tidy(tarsus_lm)
summary(tarsus_lm)
# model.matrix(tarsus_lm)


# Viz the model
# emmeans
tarsus_em <- emmeans(tarsus_lm,
                   spec = ~c(`Age (days)`, `Exp. Temp. (degree C)`))
tarsus_em

# viz the emmeans
as.data.frame(tarsus_em) |> 
  ggplot(aes(x = `Age (days)`,
             y = emmean)) + 
  geom_point(aes(color = `Exp. Temp. (degree C)`)) +
  scale_colour_manual(values = c("blue", "red")) + 
  guides(color = guide_legend(reverse = TRUE)) +
  geom_linerange(aes(ymin = lower.CL,
                     ymax = upper.CL),
                 color = "grey",
                 alpha = 0.7) +
  labs(title = "Post-Hatch Temperature as a Determinant of Quail Tarsus Length",
       subtitle = "Warmer post-hatch environments may moderately increase tarsus length",
       y = "Modeled Mean Tarsus length (mm)",
       caption = "Figure 1 - Error bars indicate 95% CI of modeled marginal means") +
  theme(plot.caption.position = "plot",
        plot.caption = element_text(hjust = 1, vjust = 1))
  


tarsus_em_temp <- emmeans(tarsus_lm,
                   spec = ~`Exp. Temp. (degree C)`)

tarsus_em_temp

as.data.frame(tarsus_em_temp) |>
  ggplot(aes(x = `Exp. Temp. (degree C)`,
             y = emmean)) +
  geom_point() +
  geom_linerange(aes(ymin = lower.CL,
                     ymax = upper.CL),
                 color = "red") +
  labs(title = "Post-Hatch Temperature as a Determinant of Quail Tarsus Length",
       subtitle = "Overall, warmer post-hatch environments appear to increase tarsus length",
       y = "Modeled Mean Tarsus length (mm)",
       caption = "Figure 2 - Error bars indicate 95% CI of modeled marginal means") +
  theme(plot.caption.position = "plot",
        plot.caption = element_text(hjust = 1, vjust = 1))
```
**So to answer the question** - It does appear that temperature affects the development of tarsus length overall, as demonstrated by Figure 2 above. However, Figure 1 reveals that at any given age, we don't have a lot of confidence that the mean tarsus length will be different, although this confidence appears to increase with age.




## 3. Something Generalized  
**In their 2011 paper, Stanton-Geddes and Anderson assessed the role of a facultative mutualism between a legume and soil rhizobia in limiting the plant’s range. After publishing, they deposited their data at Dryad http://datadryad.org/resource/doi:10.5061/dryad.8566. As part of their lab experiment, they looked at a variety of plant properties after growing plants with rhizobia from different regions. We want to look at what happened in that experiment during the March 12th sampling event.**  

### 3a. Fit a glm  
**Load the data. Vizualize. Then, fit and evaluate a generalized linear model with your choice of error distribution looking at the effect of rhizobial region and plant height as measured on march 12 as predictors of # of leaves on march 12. Does your model meet assumptions? If not, refit with a different. Why did you chose this (or these) error distribution(s)?**  
```{r}
# Load in the data set
plant_data <- read_csv("data/greenhouse_inoculation_expt_2010.csv")
# plant_data <- read_csv("https://datadryad.org/downloads/file_stream/66224")

# str(plant_data)
# summary(plant_data)
# visdat::vis_dat(plant_data)

# Visualize # of leaves by rhizobial region and plant height

plant_plot <- ggplot(data = plant_data,
                     mapping = aes(x = height_mar12,
                                   y = leaf_mar12)) +
  geom_point()

plant_plot

plant_plot +
  facet_wrap(~rhiz_region)

ggplot(data = plant_data,
      mapping = aes(x = rhiz_region,
                    y = leaf_mar12)) +
  geom_boxplot() +
  geom_jitter(aes(color = height_mar12)) +
  scale_color_distiller(palette = "Greens")


# Determine best error distribution
# Predicted data is count data, and variance appears to increase with mean. 
# Try poisson distribution with log link


plant_lm <- lm(leaf_mar12 ~ height_mar12 + rhiz_region,
                 data = plant_data)



# Fit a model # of leaves ~ rhizobial region and plant 
plant_glm <- glm(leaf_mar12 ~ height_mar12 + rhiz_region,
                 data = plant_data,
                 family = poisson(link = "log"))

# plant_glm <- glmmTMB(leaf_mar12 ~ height_mar12 + rhiz_region,
#                  data = plant_data,
#                  family = nbinom2(link = "log"))


# Check assumptions
check_model(plant_glm)
simulateResiduals(plant_glm) |> plot()
# It's not perfect, but honestly looks pretty good. 
# I also tried negative binomial distribution but it didn't look any better. 
# I tried a Gamma distribution, and it did seem better, but I kept it as a Poisson distribution 
# because the predicted value is discrete count data, not continuous, so I wasn't sure if
# Gamma would be appropriate
```


### 3b. Evaluate your treatments  
**Which rhizobial regions enable more leaves relative to the control? And in what direction?**  
```{r}
# tidy(plant_glm)
# r2(plant_glm)

# Use emmeans for marginal means be rhiz_region
plant_means <- emmeans(plant_glm,
                      specs = ~rhiz_region,
                      type = "response")

# See how predicted mean for c controls to other regions
plant_means
plant_means |> plot()

# Check out pairwise comparisons
plant_means |> contrast("pairwise") |> confint()
```


It appears that each of ther other three rhizobial regions (edge, interior, and beyond) all enable more leaves relative to the control region (c). More leaves, so direction is positive.



### 3c. Prediction intervals from a distribution  
**So, your distribution has quantiles (right? We see these in QQ plots). You can see what the value for those quantiles are from the `q*` function for a distribution. For example, the 95th percentile of a poisson with a $\lambda$ of 5 spans from 1 to 10. You can see this with `qpois(0.025, lambda = 5)` for the lower, and change it to 0.975 for the upper. Check this out. Plot the upper and lower bounds of the 95th percentile of a Poisson distribution over a range of lambdas from 1 to 30. Do the same for a negative binomial with means from 1 to 30. Note, to translate from a mean ($\mu$) with a size of 10 (you might have to look at the helpfile for qnbinom here).**  
```{r}
# Plot the upper and lower bounds of the 95th percentile of a Poisson distribution over a range of lambdas from 1 to 30
pois_data <- tibble(lambdas = seq(from = 1, to = 30, by = 1),
                    lower = qpois(0.025, lambda = lambdas),
                    upper = qpois(0.975, lambda = lambdas))

# Line range plot showing range from upper to lower by lambda
ggplot(pois_data) +
  geom_linerange(aes(x = lambdas,
                     ymin = lower,
                     ymax = upper)) +
  labs(title = "95% Reference Interval for Poisson Distribution",
       x = "Value of Lambda") +
  theme(plot.title = element_text(hjust = 0.5))


# Do the same for a negative binomial with means from 1 to 30
nbinom_data <- tibble(means = seq(from = 1, to = 30, by = 1),
                    lower = qnbinom(0.025, mu = means, size = 10),
                    upper = qnbinom(0.975, mu = means, size = 10))

# Line range plot showing range from upper to lower by lambda
ggplot(nbinom_data) +
  geom_linerange(aes(x = means,
                     ymin = lower,
                     ymax = upper)) +
  labs(title = "95% Reference Interval for Negative Binomial Distribution at Size = 10",
       x = "Mean") +
  theme(plot.title = element_text(hjust = 0.5))
```




###  3d. Prediction intervals from your model  
**All right! Armed with this knowledge, one of the frustrating things about `broom::augment()` for glm models is that it doesn’t have an `interval` argument. And has one trick. One - you need to see what scale your answer is returned on. We want to look at the response scale - the scale of the data. Second, while you can use `se_fit` to get standard errors, you don’t get a CI per se (although, hey, ~$2*se = CI$).**  

**AND - we just looked at how when you have an estimated value, you can get the prediction CI yourself by hand in the previous part of the question. So, using your creativity, visualize the fit, 95% fit interval, and 95% prediction interval at the min, mean, and max of height for each treatment. Geoms can be anything you like! Have fun here!**  
```{r}
# Visualize the fit, 95% fit interval, and 95% prediction interval at the mean

# Calculate SE of fitted values on the response scale
preds <- predict(plant_glm, type = "response", se.fit = TRUE)

# Create data frame with model data, fitted values, calculated SE, 
# and manually calculated PIs
plant_glm_data <- tibble(augment(plant_glm, type.predict = "response"),
                         pred_lower = qpois(0.025, lambda = .fitted),
                         pred_upper = qpois(0.975, lambda = .fitted),
                         se = preds$se.fit)


# Create data frame with min, max, and median height values for each category
plant_glm_data_summary <- plant_glm_data |>
  group_by(rhiz_region) |>
  filter(height_mar12 == min(height_mar12) |
         height_mar12 == max(height_mar12) |
         height_mar12 == median(height_mar12))
# Note - I wasn't sure how to do this with the mean so I tried with the median... of course that also
# didn't work where there were an even number of values, so the plot below is missing 
# one of the center PI bars.

ggplot(data = plant_glm_data,
       mapping = aes(x = height_mar12)) +
  geom_point(aes(y = leaf_mar12)) +
  geom_line(aes(y = .fitted),
            linewidth = 1.5,
            color = "green", 
            alpha = 0.5) +
  geom_ribbon(aes(ymin = .fitted - 2*se, ymax = .fitted + 2*se),
              alpha = 0.3) + 
  geom_linerange(data = plant_glm_data_summary,
                 aes(ymin = pred_lower, ymax = pred_upper),
              alpha = 0.3) +
  facet_wrap(~rhiz_region) +
  labs(title = "Predicted Leaf Counts Plotted over Actual Data",
       caption = "Plot is facetted by treatment. Ribbons indicate 95% confidence interval.
       Line ranges indicate 95% prediction intervals at min, median, and max.",
       x = "Plant Height (Inches) on March 12th",
       y = "Predicted March 12th Leaf Count") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5), 
        plot.caption = element_text(hjust = 0.5, vjust = -1))
```




### 3e. INMPRESS YOURSELF! Prediction intervals from your model  
**Again, totally optional, but, check out the sinterval package which you’d have to install from github. It uses fit models and it’s two core functions `add_fitted_sims()` and `add_predicted_sims()` to get simulated values from fit models using one or the other interval. Do this, and visualize the same thing as above however you’d like (maybe look at `ggdist`?). Or try something new? Perhaps visualize across a range of heights, and not just three?**  




## 4. Mix it up!  
**To explore the consequences of random effects, we are going to look at an example from Vonesh, J. R. and Bolker, B. M. (2005). Compensatory larval responses shift trade-offs associated with predator-induced hatching plasticity. Ecology, 86:1580–1591. In the paper, one thing they explore is the effect of tank size, predator presence, and prey density on larval tadpoles. They go on to look at induced plastic defenses in those tadpoles. It’s a cool paper, and worth a look! But, it also shows something really interesting about logistic regression models - namely, when should we consider them as mixed models.**  



### 4a. Load it up and plot  
Load the data with the following code:
```{r}
reedfrogs <- read_delim("https://github.com/rmcelreath/rethinking/raw/master/data/reedfrogs.csv",
                        delim = ";") |>
  mutate(tank = 1:n() |> as.character(),
         died = density - surv)
```


**To get used to the data, plot survivorship as a function of tank size and predator treatment. Then, to challenge yourself a bit. Expand the data to 1s and 0s (lived or died) as follows:**  

```{r}
# Plot survivorship as a function of tank size and predator treatment
ggplot(data = reedfrogs,
       mapping = aes(x = size,
                     y = surv,
                     color = pred)) +
  geom_violin() +
  geom_jitter(alpha = 0.5, width = 0.3, height = 0)


reedfrogs_expanded <- reedfrogs |>
  group_by(tank, size, pred, density) |>
  reframe(status = c(rep(1, surv), rep(0, died))) |>
  ungroup()
```



**Now with the expanded data, plot it showing who lived, who died (who tells their story…), and how that corresponds to treatment AND tank. Do you see anything to do with within-tank variability?**  
```{r}
# I am interpreting "showing who lived, who died" to mean plot every single data point, 0 or 1...
ggplot(reedfrogs_expanded,
       aes(x = factor(as.numeric(tank)),
           y = as.character(status),
           color = size)) +
  geom_jitter(width = 0) +
  facet_wrap(~pred) +
  labs(title = "Individual Survival of Larval Tidepoles", 
       subtitle = "Each dot represents an individual - plot facetted by predator presence",
       x = "Tank Number",
       y = "Survival Status",
       color = "Tank Size") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 5),
        plot.title = element_text(hjust = 0.5))
```
It seems to be that there is much more within tank variability when predators are not present

### 4b. Are you over it?  
**Often, we treat data like this as a binomial logistic regression. We look at each tank as a set of ‘coin flips’ - some living some dead. Fit a glm here with survivorship determined by `pred*size`, and then evaluate it for overdispersion. Careful to weight for number of individuals stocked in a tank (density)! What do you see when you look at overdispersion?**  
```{r}
# Fit a glm with survivorship determined by `pred*size`
tadpole_glm <- glm(status ~ pred*size,
                   weights = density,
                   family = binomial(link = "logit"),
                   data = reedfrogs_expanded)

# Evaluate for overdispersion. Careful to weight for number of individuals stocked in a 
# tank (density)!

check_overdispersion(tadpole_glm)
# Overdispersion is detected

# check_overdispersion(tadpole_glm) |> plot()
# ^Doesn't work the same as in class...

simulateResiduals(tadpole_glm) |> plot() 
simulateResiduals(tadpole_glm) |> plotQQunif()
```
When we look at overdispersion it appears that the modeled and observed residuals don't align at all.




### 4c. Fixed or Random Intercept  
**One way to fix this problem is a quasi-binomial. But, this is unsatisfying, as it’s a posthoc adjustment and not figured into the likelihood. But, another is to think in this case about what is happening in each tank. Fit a mixed model version of 5b, and assess its assumptions. How does it compare in overdispersion? Why the difference? What is a random tank effect doing here? And why couldn’t we use a FE of tank with this `pred*size` model (try it if you don’t believe me - the model will fit, but something will be… off)?**  
```{r}
# Fit a mixed model version of 5b
tadpole_mixed <- glmer(status ~ pred*size + (1 | tank),
                       weights = density,
                       family = binomial(link = "logit"),
                       data = reedfrogs_expanded)

# Check assumptions (including overdispersion)
check_model(tadpole_mixed)


check_overdispersion(tadpole_mixed)
# Overdispersion is still detected 

# check_overdispersion(tadpole_glm) |> plot()
# ^Doesn't work the same as in class...

simulateResiduals(tadpole_mixed) |> plot() 
simulateResiduals(tadpole_mixed) |> plotQQunif()
```
Overdispersion still looked terrible. I must be doing something wrong, but no matter how I change the model, if I include weights = density as was requested, the model doesn't seem to pass assumptions. 


### 4d. Changes in Parameters  
**Now that you have a model with and without a random intercept, how do the estimates of fixed effects change? Why**  
```{r}
tidy(tadpole_glm)
tidy(tadpole_mixed)
```
The estimate of intercept is notably different, which makes sense since the intercept is allowed to vary by tank in the model with a random intercept. The other fixed effects (slopes of predicters and interaction) are very similar, but slightly stronger (absolute values of estimates are larger) for the model with the random intercept. In particular the sizesmall intercept is much stronger in the mixed mode.
Perhaps the slopes are stronger in the mixed model because by isolating what's an effect of tank, the real impact of each predictor can more accurately be estimated.



### 4e. Model evaluation  
**Last, let’s evaluate this model - both with contrasts and a crisp visualization of results. `emmeans`, `modelbased`, and other friends might help you here. Or not! You do you, and extract things from the model with other helpers! There’s no one right way to do this, but, make it sing.** 
```{r}
emmeans(tadpole_mixed, specs = ~pred*size, type = "response")
emmeans(tadpole_mixed, specs = ~pred | size, type = "response")
emmeans(tadpole_mixed, specs = ~size | pred, type = "response")
# These all say the same thing, but basically predict that survival is likely when there is no 
# predator, and when there is a predator, the survival rate is predicted to be higher
# in a small tank (which is a little counterintuative)


# Plot predicted values and 95% CI with estimate_relation
estimate_relation(tadpole_mixed) |> plot() +
  labs(title = "Predicted Mean Chance of Tadpole Survival",
       subtitle = "with 95% confidence intervals",
       x = "Presence of Predator",
       y = "Predicted Mean Chance of Survival",
       color = "Tank Size") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))


```



### 4f. IMPRESS YOURSELF! Variable slopes  
**Explore multiple different RE structures beyond a variable intercept. What errors do you get? What do they mean? Can you implement some of the suggested solutions, and what do they tell you about why more complex structures don’t work well here and/or might not be appropriate. Do these different RE structures make a difference for your evaluation of the FE? What RE structure would you use? What if you added density as a covariate/predictor as in the paper?**  




## 5. What did we do  
**Write out the model equations for the glmm you have fit in 4.3. Use LaTeX to write it all out. Explain what your symbols mean in text.**  

$logit(P(status_{ij} = 1)) = \beta_0 + \beta_1pred_{ij} + \beta_2size_{ij} + \beta_3(pred_{ij} \times size_{ij}) +  \alpha_j + \epsilon_{ij}$  

$\alpha_j \sim \mathcal{N}(0, \sigma^2_\alpha)$  

$\epsilon_{ij} \sim \mathcal{N}(0, \sigma^2)$  


where $i$ is an idividual observation, $j$ represents the tank, and $/alpha$ is the random effect of the tank on the intercept.



# Midterm Self-Evaluation  



### A. How are you doing?  
I thought I was doing well but this midterm took me an incredible amount of time, much of it just staring at the screen feeling confused about the questions. I'm not feeling confident.  


### B. What concepts do you think you’ve really mastered (or are on the journey to mastery - I know, you’re still learning) in this class?  
The basic linear model! I feel like I conceptually understand the glms and mixed models, but then when it comes to applying them I just get lost. But I'm on the way.  


### C. What in this class do you find easy?  
Very little.  


### D. How would you describe your personal journey with learning to code?  
Satisfying growth in some directions, painful, slow growth in others.  
I like the procedural neatness of code, but I am feeling frustrated because I feel like it's supposed to make things more efficient, but everything I do feels excruciatingly slow because when I'm applying concepts I'm not an expert in while trying to code, everything is just so slow.  


### E. Where do you see applying coding in your life outside of just stats?  
Well, other data analysis, but I guess that's stats.  
I could see myself making small apps for certain work/academic projects in the future, for myself or others to interact with I suppose. I'm not sure in what form.  



### F. Where do you see the most opportunities for growth in your abilities? 
Concepts, and familiarity with all the packages that are out there to save me time. Also, I really need to improve my ability to garner relevant inforation from the help section in R - I use it a lot, but often get confused by the language.  


### G. Talk about your work in this course. What have you done? What haven’t you done? How has this been helpful to your growth - or not?  
I review the labs, I google (a lot). I stare at the screen. I haven't been great about working with others or going to office hours, I'll admit. I think the independent trouble shooting is good for my growth in some ways, but I really need to be interacting with other more to contextualize this skillset.  



### H. Did you find yourself stretching your abilities in this exam? Or did it just feel like wrote comfortable work? Tell me about it.  
Honestly, I felt confused by the questions often. Overall this exam took me forever, so I would say yes my abilities (and comprehension) were stretched. It was not comfortable. I generally follow the labs, but often it seemed like there were conceptual leaps that needed to be made from the labs, and I aparently wasn't quite ready to make those confortably.  


### I. How would you assess yourself from this exam - weak/sufficient/strong. Why?  
Sufficient. I don't think some of my models are how they should be, and I didn't evaluate and visualize to the quality/completeness that I would have liked to because I had just spent so much time already and needed to be done.  



### J. How would you assess yourself on the first half of this course - weak/sufficient/strong? Why?
Strong minus? I mean I felt really strong, until this exam. Even though I think I've followed the class well and done well on assignments, I was not able to apply the concepts as well as I would have liked to on the exam. And I need to be able to do these things faster!  



### K. What goals do you have for yourself for the rest of the course? What do you hope to accomplish, and how will this move you forward?  
Get faster at some of the basic stuff, and really just be able to understand the foundational math a bit more, and when/how/why to use certain approaches in R. 




